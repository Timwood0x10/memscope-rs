//! å¹¶è¡Œåˆ†ç‰‡å¤„ç†å™¨ - é«˜æ€§èƒ½å¹¶è¡Œ JSON åºåˆ—åŒ–
//!
//! è¿™ä¸ªæ¨¡å—å®ç°äº†å¹¶è¡Œåˆ†ç‰‡å¤„ç†åŠŸèƒ½ï¼Œå°†å¤§é‡åˆ†é…æ•°æ®åˆ†ç‰‡åå¹¶è¡Œå¤„ç†ï¼Œ
//! æ˜¾è‘—æé«˜ JSON åºåˆ—åŒ–çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ ¸ç³»ç»Ÿä¸Šã€‚

use crate::core::types::{AllocationInfo, TrackingError, TrackingResult};
use crate::export::data_localizer::LocalizedExportData;
use rayon::prelude::*;
use serde_json;
use std::sync::atomic::{AtomicUsize, Ordering};
use std::time::Instant;

/// å¹¶è¡Œåˆ†ç‰‡å¤„ç†å™¨é…ç½®
#[derive(Debug, Clone)]
pub struct ParallelShardConfig {
    /// æ¯ä¸ªåˆ†ç‰‡çš„å¤§å°ï¼ˆåˆ†é…æ•°é‡ï¼‰
    pub shard_size: usize,
    /// å¹¶è¡Œå¤„ç†çš„é˜ˆå€¼ï¼ˆè¶…è¿‡æ­¤æ•°é‡æ‰å¯ç”¨å¹¶è¡Œï¼‰
    pub parallel_threshold: usize,
    /// æœ€å¤§çº¿ç¨‹æ•°ï¼ˆNone è¡¨ç¤ºä½¿ç”¨ç³»ç»Ÿé»˜è®¤ï¼‰
    pub max_threads: Option<usize>,
    /// æ˜¯å¦å¯ç”¨æ€§èƒ½ç›‘æ§
    pub enable_monitoring: bool,
    /// é¢„ä¼°æ¯ä¸ªåˆ†é…çš„ JSON å¤§å°ï¼ˆç”¨äºé¢„åˆ†é…ï¼‰
    pub estimated_json_size_per_allocation: usize,
}

impl Default for ParallelShardConfig {
    fn default() -> Self {
        Self {
            shard_size: 1000,                    // æ¯ä¸ªåˆ†ç‰‡ 1000 ä¸ªåˆ†é…
            parallel_threshold: 2000,            // è¶…è¿‡ 2000 ä¸ªåˆ†é…æ‰å¹¶è¡Œ
            max_threads: None,                   // ä½¿ç”¨ç³»ç»Ÿé»˜è®¤çº¿ç¨‹æ•°
            enable_monitoring: true,             // å¯ç”¨æ€§èƒ½ç›‘æ§
            estimated_json_size_per_allocation: 200, // æ¯ä¸ªåˆ†é…çº¦ 200 å­—èŠ‚ JSON
        }
    }
}

/// å¤„ç†åçš„åˆ†ç‰‡æ•°æ®
#[derive(Debug, Clone)]
pub struct ProcessedShard {
    /// åºåˆ—åŒ–åçš„ JSON æ•°æ®
    pub data: Vec<u8>,
    /// åˆ†ç‰‡ä¸­çš„åˆ†é…æ•°é‡
    pub allocation_count: usize,
    /// åˆ†ç‰‡ç´¢å¼•
    pub shard_index: usize,
    /// å¤„ç†è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
    pub processing_time_ms: u64,
}

/// å¹¶è¡Œå¤„ç†ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone)]
pub struct ParallelProcessingStats {
    /// æ€»åˆ†é…æ•°é‡
    pub total_allocations: usize,
    /// åˆ†ç‰‡æ•°é‡
    pub shard_count: usize,
    /// ä½¿ç”¨çš„çº¿ç¨‹æ•°
    pub threads_used: usize,
    /// æ€»å¤„ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub total_processing_time_ms: u64,
    /// å¹³å‡æ¯ä¸ªåˆ†ç‰‡çš„å¤„ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub avg_shard_processing_time_ms: f64,
    /// å¹¶è¡Œæ•ˆç‡ï¼ˆç›¸å¯¹äºå•çº¿ç¨‹çš„åŠ é€Ÿæ¯”ï¼‰
    pub parallel_efficiency: f64,
    /// ååé‡ï¼ˆåˆ†é…/ç§’ï¼‰
    pub throughput_allocations_per_sec: f64,
    /// æ˜¯å¦ä½¿ç”¨äº†å¹¶è¡Œå¤„ç†
    pub used_parallel_processing: bool,
    /// æ€»è¾“å‡ºå¤§å°ï¼ˆå­—èŠ‚ï¼‰
    pub total_output_size_bytes: usize,
}

/// å¹¶è¡Œåˆ†ç‰‡å¤„ç†å™¨
pub struct ParallelShardProcessor {
    /// é…ç½®
    config: ParallelShardConfig,
    /// å¤„ç†è®¡æ•°å™¨ï¼ˆç”¨äºç›‘æ§ï¼‰
    processed_count: AtomicUsize,
}

impl ParallelShardProcessor {
    /// åˆ›å»ºæ–°çš„å¹¶è¡Œåˆ†ç‰‡å¤„ç†å™¨
    pub fn new(config: ParallelShardConfig) -> Self {
        // å¦‚æœæŒ‡å®šäº†æœ€å¤§çº¿ç¨‹æ•°ï¼Œè®¾ç½® rayon çº¿ç¨‹æ± 
        if let Some(max_threads) = config.max_threads {
            rayon::ThreadPoolBuilder::new()
                .num_threads(max_threads)
                .build_global()
                .unwrap_or_else(|e| {
                    eprintln!("âš ï¸ æ— æ³•è®¾ç½®çº¿ç¨‹æ± å¤§å°ä¸º {}: {}", max_threads, e);
                });
        }

        Self {
            config,
            processed_count: AtomicUsize::new(0),
        }
    }

    /// å¹¶è¡Œå¤„ç†åˆ†é…æ•°æ®
    pub fn process_allocations_parallel(
        &self,
        data: &LocalizedExportData,
    ) -> TrackingResult<(Vec<ProcessedShard>, ParallelProcessingStats)> {
        let start_time = Instant::now();
        let allocations = &data.allocations;

        println!(
            "ğŸ”„ Starting parallel shard processing for {} allocations...",
            allocations.len()
        );

        // å†³å®šæ˜¯å¦ä½¿ç”¨å¹¶è¡Œå¤„ç†
        let use_parallel = allocations.len() >= self.config.parallel_threshold;
        let actual_threads = if use_parallel {
            rayon::current_num_threads()
        } else {
            1
        };

        println!(
            "   Parallel mode: {}, threads: {}, shard size: {}",
            if use_parallel { "enabled" } else { "disabled" },
            actual_threads,
            self.config.shard_size
        );

        // é‡ç½®è®¡æ•°å™¨
        self.processed_count.store(0, Ordering::Relaxed);

        // å°†æ•°æ®åˆ†ç‰‡
        let shards: Vec<&[AllocationInfo]> = allocations
            .chunks(self.config.shard_size)
            .collect();

        println!("   Shard count: {}", shards.len());

        // å¹¶è¡Œæˆ–ä¸²è¡Œå¤„ç†åˆ†ç‰‡
        let processed_shards: TrackingResult<Vec<ProcessedShard>> = if use_parallel {
            shards
                .into_par_iter()
                .enumerate()
                .map(|(index, shard)| self.process_shard_optimized(shard, index))
                .collect()
        } else {
            shards
                .into_iter()
                .enumerate()
                .map(|(index, shard)| self.process_shard_optimized(shard, index))
                .collect()
        };

        let processed_shards = processed_shards?;
        let total_time = start_time.elapsed();

        // è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        let stats = self.calculate_processing_stats(
            &processed_shards,
            allocations.len(),
            actual_threads,
            total_time.as_millis() as u64,
            use_parallel,
        );

        // æ‰“å°æ€§èƒ½ç»Ÿè®¡
        self.print_performance_stats(&stats);

        Ok((processed_shards, stats))
    }

    /// ä¼˜åŒ–çš„åˆ†ç‰‡å¤„ç†æ–¹æ³•
    fn process_shard_optimized(
        &self,
        shard: &[AllocationInfo],
        shard_index: usize,
    ) -> TrackingResult<ProcessedShard> {
        let shard_start = Instant::now();

        // é¢„ä¼°è¾“å‡ºå¤§å°å¹¶é¢„åˆ†é…ç¼“å†²åŒº
        let estimated_size = shard.len() * self.config.estimated_json_size_per_allocation;
        let mut output_buffer = Vec::with_capacity(estimated_size);

        // ä½¿ç”¨ serde_json çš„é«˜æ•ˆ API ç›´æ¥åºåˆ—åŒ–åˆ°å­—èŠ‚å‘é‡
        // è¿™æ¯”æ‰‹åŠ¨æ ¼å¼åŒ–æ›´å¯é ï¼Œæ€§èƒ½ä¹Ÿå¾ˆå¥½
        serde_json::to_writer(&mut output_buffer, shard)
            .map_err(|e| TrackingError::ExportError(format!("åˆ†ç‰‡ {} åºåˆ—åŒ–å¤±è´¥: {}", shard_index, e)))?;

        let processing_time = shard_start.elapsed();

        // æ›´æ–°å¤„ç†è®¡æ•°å™¨
        self.processed_count.fetch_add(shard.len(), Ordering::Relaxed);

        // å¦‚æœå¯ç”¨ç›‘æ§ï¼Œæ‰“å°è¿›åº¦
        if self.config.enable_monitoring && shard_index % 10 == 0 {
            let _processed = self.processed_count.load(Ordering::Relaxed);
            println!(
                "   Shard {} completed: {} allocations, {} bytes, {:?}",
                shard_index,
                shard.len(),
                output_buffer.len(),
                processing_time
            );
        }

        Ok(ProcessedShard {
            data: output_buffer,
            allocation_count: shard.len(),
            shard_index,
            processing_time_ms: processing_time.as_millis() as u64,
        })
    }

    /// è®¡ç®—å¤„ç†ç»Ÿè®¡ä¿¡æ¯
    fn calculate_processing_stats(
        &self,
        shards: &[ProcessedShard],
        total_allocations: usize,
        threads_used: usize,
        total_time_ms: u64,
        used_parallel: bool,
    ) -> ParallelProcessingStats {
        let total_output_size: usize = shards.iter().map(|s| s.data.len()).sum();
        let avg_shard_time: f64 = if !shards.is_empty() {
            shards.iter().map(|s| s.processing_time_ms as f64).sum::<f64>() / shards.len() as f64
        } else {
            0.0
        };

        let throughput = if total_time_ms > 0 {
            (total_allocations as f64 * 1000.0) / total_time_ms as f64
        } else {
            0.0
        };

        // ä¼°ç®—å¹¶è¡Œæ•ˆç‡ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
        let parallel_efficiency = if used_parallel && threads_used > 1 {
            // ç†æƒ³æƒ…å†µä¸‹ï¼ŒN ä¸ªçº¿ç¨‹åº”è¯¥æä¾›æ¥è¿‘ N å€çš„æ€§èƒ½
            // å®é™…æ•ˆç‡ = å®é™…åŠ é€Ÿæ¯” / ç†è®ºåŠ é€Ÿæ¯”
            let theoretical_speedup = threads_used as f64;
            let estimated_sequential_time = avg_shard_time * shards.len() as f64;
            let actual_speedup = if total_time_ms > 0 {
                estimated_sequential_time / total_time_ms as f64
            } else {
                1.0
            };
            (actual_speedup / theoretical_speedup).min(1.0)
        } else {
            1.0 // å•çº¿ç¨‹æ•ˆç‡ä¸º 100%
        };

        ParallelProcessingStats {
            total_allocations,
            shard_count: shards.len(),
            threads_used,
            total_processing_time_ms: total_time_ms,
            avg_shard_processing_time_ms: avg_shard_time,
            parallel_efficiency,
            throughput_allocations_per_sec: throughput,
            used_parallel_processing: used_parallel,
            total_output_size_bytes: total_output_size,
        }
    }

    /// æ‰“å°æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
    fn print_performance_stats(&self, stats: &ParallelProcessingStats) {
        println!("âœ… Parallel shard processing completed:");
        println!("   Total allocations: {}", stats.total_allocations);
        println!("   Shard count: {}", stats.shard_count);
        println!("   Threads used: {}", stats.threads_used);
        println!("   Total time: {}ms", stats.total_processing_time_ms);
        println!("   Average shard time: {:.2}ms", stats.avg_shard_processing_time_ms);
        println!("   Throughput: {:.0} allocations/sec", stats.throughput_allocations_per_sec);
        println!("   Output size: {:.2} MB", stats.total_output_size_bytes as f64 / 1024.0 / 1024.0);
        
        if stats.used_parallel_processing {
            println!("   Parallel efficiency: {:.1}%", stats.parallel_efficiency * 100.0);
            let speedup = stats.parallel_efficiency * stats.threads_used as f64;
            println!("   Actual speedup: {:.2}x", speedup);
        }
    }

    /// è·å–å½“å‰é…ç½®
    pub fn get_config(&self) -> &ParallelShardConfig {
        &self.config
    }

    /// æ›´æ–°é…ç½®
    pub fn update_config(&mut self, config: ParallelShardConfig) {
        self.config = config;
    }

    /// è·å–å¤„ç†è¿›åº¦
    pub fn get_processed_count(&self) -> usize {
        self.processed_count.load(Ordering::Relaxed)
    }
}

impl Default for ParallelShardProcessor {
    fn default() -> Self {
        Self::new(ParallelShardConfig::default())
    }
}

/// ä¾¿åˆ©å‡½æ•°ï¼šå¿«é€Ÿå¹¶è¡Œå¤„ç†åˆ†é…æ•°æ®
pub fn process_allocations_fast(
    data: &LocalizedExportData,
) -> TrackingResult<(Vec<ProcessedShard>, ParallelProcessingStats)> {
    let processor = ParallelShardProcessor::default();
    processor.process_allocations_parallel(data)
}

/// ä¾¿åˆ©å‡½æ•°ï¼šä½¿ç”¨è‡ªå®šä¹‰é…ç½®å¹¶è¡Œå¤„ç†
pub fn process_allocations_with_config(
    data: &LocalizedExportData,
    config: ParallelShardConfig,
) -> TrackingResult<(Vec<ProcessedShard>, ParallelProcessingStats)> {
    let processor = ParallelShardProcessor::new(config);
    processor.process_allocations_parallel(data)
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::core::types::{MemoryStats, ScopeInfo};
    use crate::analysis::unsafe_ffi_tracker::UnsafeFFIStats;
    use std::time::Instant;

    fn create_test_data(allocation_count: usize) -> LocalizedExportData {
        let mut allocations = Vec::new();
        for i in 0..allocation_count {
            allocations.push(AllocationInfo {
                ptr: 0x1000 + i,
                size: 64 + (i % 100),
                type_name: Some(format!("TestType{}", i % 10)),
                var_name: Some(format!("var_{}", i)),
                scope_name: Some(format!("scope_{}", i % 5)),
                timestamp_alloc: 1000000 + i as u64,
                timestamp_dealloc: None,
                thread_id: format!("test_thread_{}", i % 3),
                borrow_count: 0,
                stack_trace: None,
                is_leaked: false,
                lifetime_ms: None,
                smart_pointer_info: None,
                memory_layout: None,
                generic_info: None,
                dynamic_type_info: None,
                runtime_state: None,
                stack_allocation: None,
                temporary_object: None,
                fragmentation_analysis: None,
                generic_instantiation: None,
                type_relationships: None,
                type_usage: None,
                function_call_tracking: None,
                lifecycle_tracking: None,
                access_tracking: None,
            });
        }

        LocalizedExportData {
            allocations,
            enhanced_allocations: Vec::new(),
            stats: MemoryStats::default(),
            ffi_stats: UnsafeFFIStats::default(),
            scope_info: Vec::<ScopeInfo>::new(),
            timestamp: Instant::now(),
        }
    }

    #[test]
    fn test_parallel_shard_processor_creation() {
        let config = ParallelShardConfig::default();
        let processor = ParallelShardProcessor::new(config);
        assert_eq!(processor.get_config().shard_size, 1000);
    }

    #[test]
    fn test_small_dataset_sequential_processing() {
        let data = create_test_data(100); // å°æ•°æ®é›†ï¼Œåº”è¯¥ä½¿ç”¨ä¸²è¡Œå¤„ç†
        let processor = ParallelShardProcessor::default();
        
        let result = processor.process_allocations_parallel(&data);
        assert!(result.is_ok());
        
        let (shards, stats) = result.unwrap();
        assert_eq!(stats.total_allocations, 100);
        assert!(!stats.used_parallel_processing); // åº”è¯¥ä½¿ç”¨ä¸²è¡Œå¤„ç†
        assert_eq!(shards.len(), 1); // åªæœ‰ä¸€ä¸ªåˆ†ç‰‡
    }

    #[test]
    fn test_large_dataset_parallel_processing() {
        let data = create_test_data(5000); // å¤§æ•°æ®é›†ï¼Œåº”è¯¥ä½¿ç”¨å¹¶è¡Œå¤„ç†
        let processor = ParallelShardProcessor::default();
        
        let result = processor.process_allocations_parallel(&data);
        assert!(result.is_ok());
        
        let (shards, stats) = result.unwrap();
        assert_eq!(stats.total_allocations, 5000);
        assert!(stats.used_parallel_processing); // åº”è¯¥ä½¿ç”¨å¹¶è¡Œå¤„ç†
        assert!(shards.len() > 1); // åº”è¯¥æœ‰å¤šä¸ªåˆ†ç‰‡
        
        // éªŒè¯æ‰€æœ‰åˆ†ç‰‡çš„åˆ†é…æ€»æ•°ç­‰äºåŸå§‹æ•°æ®
        let total_processed: usize = shards.iter().map(|s| s.allocation_count).sum();
        assert_eq!(total_processed, 5000);
    }

    #[test]
    fn test_custom_config() {
        let config = ParallelShardConfig {
            shard_size: 500,
            parallel_threshold: 1000,
            max_threads: Some(2),
            enable_monitoring: false,
            estimated_json_size_per_allocation: 150,
        };
        
        let data = create_test_data(2000);
        let processor = ParallelShardProcessor::new(config);
        
        let result = processor.process_allocations_parallel(&data);
        assert!(result.is_ok());
        
        let (shards, stats) = result.unwrap();
        assert_eq!(stats.total_allocations, 2000);
        assert_eq!(shards.len(), 4); // 2000 / 500 = 4 ä¸ªåˆ†ç‰‡
    }

    #[test]
    fn test_convenience_functions() {
        let data = create_test_data(1500);
        
        // æµ‹è¯•å¿«é€Ÿå¤„ç†å‡½æ•°
        let result = process_allocations_fast(&data);
        assert!(result.is_ok());
        
        // æµ‹è¯•è‡ªå®šä¹‰é…ç½®å‡½æ•°
        let config = ParallelShardConfig {
            shard_size: 300,
            ..Default::default()
        };
        let result = process_allocations_with_config(&data, config);
        assert!(result.is_ok());
        
        let (shards, _) = result.unwrap();
        assert_eq!(shards.len(), 5); // 1500 / 300 = 5 ä¸ªåˆ†ç‰‡
    }

    #[test]
    fn test_processed_shard_structure() {
        let data = create_test_data(100);
        let processor = ParallelShardProcessor::default();
        
        let result = processor.process_allocations_parallel(&data);
        assert!(result.is_ok());
        
        let (shards, _) = result.unwrap();
        assert_eq!(shards.len(), 1);
        
        let shard = &shards[0];
        assert_eq!(shard.allocation_count, 100);
        assert_eq!(shard.shard_index, 0);
        assert!(!shard.data.is_empty());
        // processing_time_ms is u64, always >= 0, so just check it exists
        assert!(shard.processing_time_ms < u64::MAX);
        
        // éªŒè¯ JSON æ•°æ®æ˜¯æœ‰æ•ˆçš„
        let parsed: Result<Vec<AllocationInfo>, _> = serde_json::from_slice(&shard.data);
        assert!(parsed.is_ok());
        assert_eq!(parsed.unwrap().len(), 100);
    }
}